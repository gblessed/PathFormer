# -*- coding: utf-8 -*-
"""Untitled34.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CD-Hu4AYz77LnpQM8KZuhFc3YO6fIdsC
"""

# %%
# !pip install DeepMIMO==4.0.0b10

# %%
# %%
# =============================================================================
# 1. IMPORTS AND WARNINGS SETUP
#    - Load necessary PyTorch modules, utilities, and suppress UserWarnings
# =============================================================================
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim.lr_scheduler import ReduceLROnPlateau
import os
import torch
from tqdm import tqdm
import math
# from utils import (generate_channels_and_labels, tokenizer_train, tokenizer, make_sample, nmse_loss,
                #    create_train_dataloader, patch_maker, count_parameters, train_lwm)
from collections import defaultdict
import numpy as np
# import pretrained_model  # Assuming this contains the LWM model definition
import matplotlib.pyplot as plt
import warnings
import os
import bisect
# from collections import defaultdict
from tqdm import tqdm
warnings.filterwarnings("ignore", category=UserWarning)
# from utils import *
import deepmimo as dm
from sklearn.metrics import mean_squared_error
import numpy as np
import random
from tqdm import tqdm
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import wandb
from models import GPTPathDecoder

# %%
scenario = 'city_23_beijing_3p5'



config = {
    "BATCH_SIZE":64,
    "PAD_VALUE": 500,
    "USE_WANDB": True,
    "LR":2e-5,
    "epochs" : 100,
    "interaction_weight": 0.01,  # Weight for interaction loss
    # "experiment": "interacaction_power_only_dec_only",
    "pre_train": False,
    "base_experiment": f"pre_train_all_scenarios_interaction_weight_0.01_better_scheduler",
    "experiment": f"finetune_{scenario}_interaction_weight_0.01",
    "finetune_scenario": "city_0_newyork_3p5",
    "hidden_dim": 512,
    "n_layers": 6,
    "n_heads": 4,
}




# %%
class MySeqDataLoader(torch.utils.data.Dataset):

    def __init__(self, scenario, tx_sets="all", seed=42, shuffle=False, pad_value=500,
                 train=True, split_by="users", train_ratio=0.8, sort_by="power"):
        ### get length of dataset
        self.dataset = scenario
        self.Txs = 1
        self.pad_value = pad_value
        self.split_by = split_by
        self.sort_by = sort_by
        self.dataset_filtered = defaultdict(list)
        self.total_length = 0
        scattering_model = { None:0, 'none':0, 'directive':1, }
        if isinstance(self.dataset.n_ue, int):
        
            self.dataset = [self.dataset]

        if split_by == "Tx":
            self.Txs = list(range(len(self.dataset)))
            if train:
                self.Txs = self.Txs[:int(train_ratio * len(self.Txs))]
            else:
                self.Txs = self.Txs[int(train_ratio * len(self.Txs)):]

            for tx in self.Txs:
                use_indices = self.dataset[tx].los != -1
                n_ue = self.dataset[tx].n_ue

                for k in self.dataset[tx]:
                    if k not in ["txrx", "load_params", "name", "rt_params", "materials", "scene", "n_ue"]:
                        if self.dataset[tx][k].shape[0] == n_ue:
                            self.dataset_filtered[k].extend(self.dataset[tx][k][use_indices].tolist())
                self.dataset_filtered["tx_pos"].extend(np.repeat(self.dataset[tx]["tx_pos"], n_ue, axis=0).tolist())

        elif split_by == "user":
            for tx in range(len(self.dataset)):
                n_ue = self.dataset[tx].n_ue

                # --- user-level splitting ---
                indices = np.arange(n_ue)
                np.random.seed(seed + tx)
                np.random.shuffle(indices)

                split_idx = int(train_ratio * len(indices))
                if train:
                    indices = indices[:split_idx]
                else:
                    indices = indices[split_idx:]

                use_indices = self.dataset[tx].los != -1
                indices = [i for i in indices if use_indices[i]]

                # Collect data
                for k in self.dataset[tx]:
                    # if k in ["txrx", "load_params", "name", "rt_params", "materials", "scene", "n_ue"]:
                    if k in ["txrx", "load_params", "name", "rt_params","n_ue",]:

                        continue
                    elif k == "scene":
                        self.dataset_filtered["num_materials"].extend( [len(self.dataset[tx][k].objects)]*len(indices))
                        self.dataset_filtered["bounding_box"].extend([abs(self.dataset[tx][k].bounding_box.bounds[1] - self.dataset[tx][k].bounding_box.bounds[0]).tolist()] * len(indices))


                    elif k == "materials":
                        all_material_types = []
                        for material in self.dataset[tx][k]:
                            building_types = []
                            for material_property in ["permittivity", "conductivity", "scattering_coefficient", "scattering_model", "thickness", "roughness"]:
                                if material_property == "scattering_model":

                                    building_types.append(scattering_model[material.__dict__[material_property]] )
                                else:
                                    building_types.append( material.__dict__[material_property] )
                            all_material_types.append(building_types)

                        self.dataset_filtered["material_properties"].extend( [all_material_types]*len(indices))

                    elif self.dataset[tx][k].shape[0] == n_ue:
                        self.dataset_filtered[k].extend(self.dataset[tx][k][indices].tolist())

                # TX position duplicated for each UE sample
                if np.array(self.dataset[tx]["tx_pos"]).ndim == 1:
                    self.dataset_filtered["tx_pos"].extend(
                        np.repeat(self.dataset[tx]["tx_pos"][np.newaxis, :], len(indices), axis=0).tolist()
                    )
                else:
                    self.dataset_filtered["tx_pos"].extend(
                        np.repeat(self.dataset[tx]["tx_pos"], len(indices), axis=0).tolist()
                    )
        self.seed = seed
        self.total_length = len(self.dataset_filtered[list(self.dataset_filtered.keys())[0]])

        # boundary = self.dataset[0]['rt_params']['raw_params']['studyarea']['boundary']['data']
        # self.mins = np.array([boundary[0][0], boundary[0][1],
        #                      self.dataset[0]['rt_params']['raw_params']['studyarea']['boundary']['values']['zmin']]).astype(np.float32)
        # self.maxs = np.array([boundary[2][0], boundary[2][1],
        #                      self.dataset[0]['rt_params']['raw_params']['studyarea']['boundary']['values']['zmax']]).astype(np.float32)

    def decode_interaction_to_multilabel(self, inter_code):
        """
        Convert interaction code to multi-label vector [R, D, S, T]
        Ignores repetitions: RRD -> RD, RR -> R

        Returns:
            np.array of shape (4,): binary indicators for [R, D, S, T]
            Returns [-1, -1, -1, -1] for invalid/NaN codes
        """
        if np.isnan(inter_code):
            return np.array([-1, -1, -1, -1], dtype=np.float32)

        code_str = str(int(inter_code))
        multi_label = np.zeros(4, dtype=np.float32)

        # Map: 1->R, 2->D, 3->S, 4->T
        for digit in code_str:
            if digit == '1':
                multi_label[0] = 1  # R
            elif digit == '2':
                multi_label[1] = 1  # D
            elif digit == '3':
                multi_label[2] = 1  # S
            elif digit == '4':
                multi_label[3] = 1  # T

        return multi_label

    def __getitem__(self, idx):
        prompt = []
        environment = []
        ## add building count
        # for k in ["num_materials", "bounding_box"]:
        #     print( f"{k} :", len(self.dataset_filtered[k]))
        environment.append(self.dataset_filtered["num_materials"][idx])
        environment.extend(self.dataset_filtered["bounding_box"][idx])
        environment_material_props = self.dataset_filtered["material_properties"][idx]


        for k in ["tx_pos", "rx_pos"]:
            prompt.extend(self.dataset_filtered[k][idx])

        # Sort paths based on sort_by option
        if self.sort_by == "power":
            indices = np.argsort(-np.array(self.dataset_filtered["power"][idx]))
        elif self.sort_by == "delay":
            indices = np.argsort(np.array(self.dataset_filtered["delay"][idx]))
        else:
            raise ValueError(f"Unknown sort_by option: {self.sort_by}")

        paths = []
        interactions = []  # NEW: multi-label interactions
        valid_paths = 0

        # SOS token
        paths.append([0.0, 0.0, 0.0])
        interactions.append([-1, -1, -1, -1])  # SOS has no interaction label

        for step_idx, indx in enumerate(indices):
            output_per_step = []
            broken = False

            for k in ["delay", "power", "phase"]:
                value = self.dataset_filtered[k][idx][indx]
                if np.isnan(value):
                    value = self.pad_value
                    broken = True
                    break
                elif k == "delay":
                    value = value * 1e6  # convert to us
                elif k == "phase":
                    value = value * (np.pi/180)
                elif k == "power":
                    value = value * 0.01

                output_per_step.append(value)

            if not broken:
                valid_paths += 1
                paths.append(output_per_step)

                # NEW: Extract and decode interaction
                inter_value = self.dataset_filtered["inter"][idx][indx]
                inter_label = self.decode_interaction_to_multilabel(inter_value)
                interactions.append(inter_label)

        num_paths = [valid_paths]


        return (torch.tensor(prompt, dtype=torch.float32),
                torch.tensor(paths, dtype=torch.float32),
                torch.tensor(num_paths, dtype=torch.float32) / 25.0,
                torch.tensor(interactions, dtype=torch.float32),
                torch.tensor(environment, dtype=torch.float32),
                torch.tensor(environment_material_props, dtype=torch.float32),)  # NEW

    def __len__(self):
        return self.total_length

    def collate_fn(self, batch):
        batch_prompts = torch.cat([i[0].unsqueeze(0) for i in batch], dim=0)
        batch_paths = [i[1] for i in batch]
        batch_paths = torch.nn.utils.rnn.pad_sequence(batch_paths, batch_first=True,
                                                       padding_value=self.pad_value)
        batch_num_paths = [i[2] for i in batch]
        batch_num_paths = torch.nn.utils.rnn.pad_sequence(batch_num_paths, batch_first=True,
                                                           padding_value=0)

        # NEW: collate interactions
        batch_interactions = [i[3] for i in batch]
        batch_interactions = torch.nn.utils.rnn.pad_sequence(batch_interactions, batch_first=True,
                                                             padding_value=-1)
        batch_environment = torch.cat([i[4].unsqueeze(0) for i in batch], dim=0)
        batch_environment_material_props = torch.cat([i[5].unsqueeze(0) for i in batch], dim=0)
        return batch_prompts, batch_paths, batch_num_paths, batch_interactions, batch_environment, batch_environment_material_props


def generate_paths(model, prompt, env, env_prop, max_steps=25, stop_threshold=0.5):
    """
    Generate paths autoregressively.
    """
    model.eval()
    prompt = prompt.unsqueeze(0).cuda()  # (1, prompt_dim)
    env = env.unsqueeze(0).cuda()  # (1, prompt_dim)
    env_prop = env_prop.unsqueeze(0).cuda()  # (1, prompt_dim)

    # Start with SOS tokens
    cur = torch.zeros(1, 1, 3).cuda()  # (1, 1, 3) - delay, power, phase
    inter_str = -1 * torch.ones(1, 1, 4).cuda()  # (1, 1, 4) - interaction labels

    outputs = []
    outputs_inter_str = []

    for t in range(max_steps):
        # Forward pass
        d, p, s, c, ph, pathcounts, inter_str_logits = model(prompt, cur, inter_str, env, env_prop)

        # Get last timestep predictions
        d_t = d[:, -1]           # (1,)
        p_t = p[:, -1]           # (1,)
        ph_t = ph[:, -1]         # (1,)
        inter_logits_t = inter_str_logits[:, -1]  # (1, 4)

        # **FIX: Convert logits to binary predictions**
        inter_pred_t = (torch.sigmoid(inter_logits_t) > 0.5).float()  # (1, 4) - binary [0, 1]

        # Store outputs
        outputs.append(torch.stack([d_t, p_t, ph_t], dim=-1))
        outputs_inter_str.append(inter_pred_t)

        # Append predictions for next iteration
        next_path = torch.stack([d_t, p_t, ph_t], dim=-1).unsqueeze(1)  # (1, 1, 3)
        cur = torch.cat([cur, next_path], dim=1)

        # **FIX: Use binary predictions, not logits**
        inter_str = torch.cat([inter_str, inter_pred_t.unsqueeze(1)], dim=1)  # (1, t+2, 4)

    return (torch.stack(outputs, dim=1).squeeze(0).detach().cpu(),  # (T, 3)
            pathcounts,
            torch.stack(outputs_inter_str, dim=1).squeeze(0).detach().cpu())  # (T, 4)




def masked_loss(delay_pred, power_pred, sin_pred, cos_pred, phase_pred,
                path_length_predict, interaction_logits, targets, path_length_targets,
                interaction_targets, pad_value=500, interaction_weight=0.1):
    """
    Added interaction prediction loss as auxiliary task.

    Args:
        interaction_logits: (B, T, 4) - logits for [R, D, S, T]
        interaction_targets: (B, T, 4) - binary labels, -1 for invalid
        interaction_weight: weight for interaction loss
    """
    delay_t, power_t, phase_t = targets[:, :, 0], targets[:, :, 1], targets[:, :, 2]
    sinp = torch.sin(phase_t)
    cosp = torch.cos(phase_t)

    # Mask for valid paths
    mask = (delay_t != pad_value)

    # Existing losses
    loss_delay = ((delay_pred - delay_t)**2)[mask].mean()
    loss_power = ((power_pred - power_t)**2)[mask].mean()
    loss_sin = ((sin_pred - sinp)**2)[mask].mean()
    loss_cos = ((cos_pred - cosp)**2)[mask].mean()
    loss_phase = (loss_sin + loss_cos) / 2

    loss_path_length = ((path_length_targets - path_length_predict)**2).mean() * 0.0

    # NEW: Multi-label interaction loss
    # Mask: valid interactions (not -1)
    interaction_mask = (interaction_targets[:, :, 0] != -1)  # (B, T)

    if interaction_mask.any():
        # Binary cross-entropy for multi-label classification
        valid_logits = interaction_logits[interaction_mask]  # (N, 4)
        valid_targets = interaction_targets[interaction_mask]  # (N, 4)

        loss_interaction = F.binary_cross_entropy_with_logits(
            valid_logits,
            valid_targets,
            reduction='mean'
        )
    else:
        loss_interaction = torch.tensor(0.0, device=delay_pred.device)

    total_loss = (loss_delay + loss_power + loss_phase +
                  loss_path_length + interaction_weight * loss_interaction)

    # total_loss = (loss_delay +
    #              + interaction_weight * loss_interaction)

    return (total_loss, loss_delay, loss_power, loss_phase,
            loss_path_length, loss_interaction)

def compute_stop_metrics(path_count, targets, pad_value=500):
    """

    Args:

    """

    rmse = np.sqrt(mean_squared_error(path_count.cpu().numpy(), targets.squeeze().cpu().numpy()))

    return rmse



def evaluate_model(model, val_loader, max_generate=26, log_to_wandb=False):
    model.eval()

    delay_errors = []
    power_errors = []
    phase_errors = []
    path_length_rmses = []



    delay_maes = []
    power_maes = []
    phase_maes = []
    path_length_maes = []
    with torch.no_grad():
        outer_bar = tqdm(val_loader, desc="Evaluating (batches)", leave=True)

        for prompts, paths, path_lengths,interactions, env, env_prop  in outer_bar:
            prompts = prompts.cuda()
            paths = paths.cuda()
            path_lengths = path_lengths.cuda()
            env = env.cuda()
            env_prop = env_prop.cuda()

            # Inner tqdm to show per-sample progress
            inner_bar = tqdm(range(prompts.size(0)),
                             desc="   Processing samples",
                             leave=False)

            for b in inner_bar:
                generated, path_lengths_pred, inter_str_pred = generate_paths(model, prompts[b],env[b], env_prop[b], max_steps=max_generate)

                generated = generated.cuda()
                gt = paths[b][1:, :3]  # only (delay, power, phase)

                # Mask padded values
                valid_mask = (gt[:,0] != train_data.pad_value)
                gt = gt[valid_mask]

                T = min(len(gt), len(generated))
                pred = generated[:T]
                gt = gt[:T]

                # ---- Compute Metrics ----
                delay_rmse = torch.mean((pred[:,0] - gt[:,0])**2).sqrt().item()
                delay_mae = torch.mean(torch.abs(pred[:,0] - gt[:,0])).item()

                power_rmse = torch.mean((pred[:,1]/0.01 - gt[:,1]/0.01)**2).sqrt().item()
                power_mae = torch.mean((torch.abs(pred[:,1]/0.01 - gt[:,1]/0.01))).item()


                # Phase errors
                y_hat_angles = (pred[:,2] / (np.pi/180))
                y_angles = (gt[:,2] / (np.pi/180))
                # phase_rmse =    (torch.mean(y_hat_angles - y_angles)**2).sqrt().item()
                phase_circular_dist = (y_hat_angles - y_angles + 180) % 360 - 180
                phase_rmse = torch.mean(phase_circular_dist**2).sqrt().item()
                phase_mae = torch.mean(torch.abs(phase_circular_dist)).item()

                # Path length RMSE
                # print(path_lengths_pred, path_lengths[b],)
                length_rmse = (torch.mean( (path_lengths_pred - path_lengths[b])**2)).sqrt().item()
                length_mae = (torch.mean(torch.abs(path_lengths_pred - path_lengths[b]))).item()


                # Save metrics
                delay_errors.append(delay_rmse)
                power_errors.append(power_rmse)
                phase_errors.append(phase_rmse)
                path_length_rmses.append(length_rmse)

                delay_maes.append(delay_mae)
                power_maes.append(power_mae)
                phase_maes.append(phase_mae)
                path_length_maes.append(length_mae)
                # Show live metric values in tqdm
                inner_bar.set_postfix({
                    "delay": f"{delay_rmse:.3f}",
                    "power": f"{power_rmse:.3f}",
                    "phase": f"{phase_rmse:.3f}",
                    "length": f"{length_rmse:.3f}",
                    "delay": f"{delay_mae:.3f}",
                    "power": f"{power_mae:.3f}",
                    "phase": f"{phase_mae:.3f}",
                    "length": f"{length_mae:.3f}"
                })

                # wandb logging
                if log_to_wandb:
                    wandb.log({
                        "test_delay_rmse": delay_rmse,
                        "test_power_rmse": power_rmse,
                        "test_phase_circ_err": phase_rmse,
                        "test_stop_length_rmse": length_rmse,
                        "test_delay_mae": delay_mae,
                        "test_power_mae": power_mae,
                        "test_phase_circ_err_mae": phase_mae,
                        "test_stop_length_mae": length_mae,
                    })
            # print("Batch evaluation complete.")

            # print("\n================= Up toBATCH EVALUATION RESULTS =================")
            # print(f"Avg Delay RMSE           : {np.mean(delay_errors):.4f} µs")
            # print(f"Avg Power RMSE           : {np.mean(power_errors):.4f} dB")
            # print(f"Avg Phase RMSE           : {np.mean(phase_errors):.4f} degrees")
            # print(f"Avg Path Length RMSE     : {np.mean(path_length_rmses):.4f}")
            # print(f"Avg Delay MAE           : {np.mean(delay_maes):.4f} µs")
            # print(f"Avg Power MAE           : {np.mean(power_maes):.4f} dB")
            # print(f"Avg Phase MAE           : {np.mean(phase_maes):.4f} degrees")
            # print(f"Avg Path Length MAE     : {np.mean(path_length_maes):.4f}")
            # print("============================================================")


    # ---- Final Aggregated Results ----
    avg_delay = np.mean(delay_errors)
    avg_power = np.mean(power_errors)
    avg_phase = np.mean(phase_errors)
    avg_path_length_rmse = np.mean(path_length_rmses)

    avg_delay_mae = np.mean(delay_maes)
    avg_power_mae = np.mean(power_maes)
    avg_phase_mae = np.mean(phase_maes)
    avg_path_length_mae= np.mean(path_length_maes)

    print("\n=================  Final EVALUATION RESULTS =================")
    print(f"Delay RMSE           : {avg_delay:.4f} µs")
    print(f"Power RMSE           : {avg_power:.4f} dB")
    print(f"Phase RMSE           : {avg_phase:.4f} degrees")
    print(f"Path Length RMSE     : {avg_path_length_rmse:.4f}")

    print(f"Delay MAE           : {avg_delay_mae:.4f} µs")
    print(f"Power MAE           : {avg_power_mae:.4f} dB")
    print(f"Phase MAE           : {avg_phase_mae:.4f} degrees")
    print(f"Path Length MAE     : {avg_path_length_mae:.4f}")
    print("=====================================================\n")

    if log_to_wandb:
        wandb.run.summary["test_delay_rmse"] = avg_delay
        wandb.run.summary["test_power_rmse"] = avg_power
        wandb.run.summary["test_phase_circ_err"] = avg_phase
        wandb.run.summary["test_path_length_rmse"] = avg_path_length_rmse

        wandb.run.summary["test_delay_mae"] = avg_delay_mae
        wandb.run.summary["test_power_mae"] = avg_power_mae
        wandb.run.summary["test_phase_circ_err_mae"] = avg_phase_mae
        wandb.run.summary["test_path_length_mae"] = avg_path_length_mae

    return avg_delay, avg_power, avg_phase, avg_path_length_rmse, avg_delay_mae, avg_power_mae, avg_phase_mae,avg_path_length_mae




def show_example(model, val_loader, sample_index=0, k=25, plot=True):
    model.eval()
    prompts, paths, path_lengths,interactions = next(iter(val_loader))

    prompts = prompts.cuda()
    paths = paths.cuda()

    pred_paths, path_lengths_pred,inter_str_pred= generate_paths(model, prompts[sample_index])


    pred = pred_paths[0]  # (T,3)
    gt = paths[sample_index][1:, :3]  # Extract only 3D components (T,3)

    valid = (gt[:,0] != train_data.pad_value)
    gt = gt[valid]

    print("\n--- Ground Truth Length {} ".format(len(gt)))

    print("\n--- Model Predict Length (first {} paths) ---".format(path_lengths_pred.item()))


    print(gt[:k])
    print(pred[:k])

    if plot:
        T = min(len(gt), len(pred))
        # print("len_path", len(pred), "actual = ", T)
        pred = pred[:T]
        gt = gt[:T]

        fig, axs = plt.subplots(3,1, figsize=(10,12))

        axs[0].plot(gt[:,0].cpu(), label="GT Delay", marker='o')
        axs[0].plot(pred[:,0].cpu(), label="Pred Delay", marker='x')
        axs[0].set_title("Path Delay (µs)")
        axs[0].legend()

        axs[1].plot(gt[:,1].cpu()*0.01, label="GT Power", marker='o')
        axs[1].plot(pred[:,1].cpu()*0.01, label="Pred Power", marker='x')
        axs[1].set_title("Path Power dB")
        axs[1].legend()

        axs[2].plot(gt[:,2].cpu()/(np.pi/180), label="GT Phase", marker='o')
        axs[2].plot(pred[:,2].cpu()/(np.pi/180), label="Pred Phase", marker='x')
        axs[2].set_title("Path Phase (degrees)")

        axs[2].legend()

        plt.tight_layout()
        plt.show()


def evaluate_generation(val_loader, n_samples=3):
    model.eval()
    for i, (prompts, paths, path_lengths, interactions, env, env_prop) in enumerate(val_loader):
        if i >= n_samples:
            break
        pred, path_lengths_pred = generate_paths(model, prompts[0],env[0], env_prop[0] )  # autoregressive generation
        print(f"path lengths pred: {path_lengths_pred[0]}")
        print(f"\nSample {i}")
        print("GT paths (first 5):")
        print(paths[0][:5])
        print("Predicted paths (first 5):")
        print(pred[0][:5])

# %%
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)




# %%


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# model = PathDecoder().to(device)
model = GPTPathDecoder(hidden_dim=config["hidden_dim"], n_layers = config["n_layers"], n_heads=config["n_heads"]).to(device)


print("Total trainable parameters:", count_parameters(model))


# %%
if config["USE_WANDB"]:
    import wandb

    wandb.init(
        project="deepmimo-path-decoder",
        config = config
       
    )


# %%


optimizer = torch.optim.AdamW(model.parameters(), lr=config["LR"])
# scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=2, mode="min")
scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
    optimizer,
    T_0=20,      # Restart every 10 epochs
    T_mult=1,    # Double the period after each restart
    eta_min=1e-8 # Minimum LR
)

# Initialize best checkpoint tracking (based on path_length loss)

# scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=2, mode="min")
pre_train = config["pre_train"]
best_val_loss = float('inf')
base_model_checkpoint_path = f"{config['base_experiment']}_best_model_checkpoint.pth"
base_model_checkpoint_path = os.path.join("checkpoints20M", base_model_checkpoint_path)
checkpoint_path = f"{config['experiment']}_best_model_checkpoint.pth"

os.makedirs("checkpoints2", exist_ok=True)
checkpoint_path = os.path.join("checkpoints2", checkpoint_path)
def train_with_interactions(model, config):
    """
    Modified training loop with interaction prediction.
    """

    best_val_loss = float('inf')
    
    train_losses = []
    train_loss_delay = []
    train_loss_power = []
    train_loss_phase = []
    train_loss_path_length = []
    train_loss_interaction = []  # NEW
    train_path_length_rmse = []

    val_losses = []
    val_loss_delay = []
    val_loss_power = []
    val_loss_phase = []
    val_loss_path_length = []
    val_loss_interaction = []  # NEW
    val_path_length_rmse = []


    for epoch in range(config["epochs"]):
         # -------------------- TRAINING --------------------
        model.train()

        pbar = tqdm(train_loader, desc=f"Epoch {epoch} [Train]", leave=False)
        for prompts, paths, path_lengths, interactions, env, env_prop in pbar:  # NEW: added interactions
            prompts = prompts.cuda()
            paths = paths.cuda()
            path_lengths = path_lengths.cuda()
            interactions = interactions.cuda()  # NEW
            env = env.cuda()
            env_prop = env_prop.cuda()

            paths_in = paths[:, :-1, :]
            interactions_in = interactions[:, :-1, :]

            paths_out = paths[:, 1:, :]
            interactions_out = interactions[:, 1:, :]  # NEW: shift targets     


            (delay_pred, power_pred, sin_pred, cos_pred, phase_pred,
            path_length_pred, interaction_logits) = model(prompts, paths_in, interactions_in, env, env_prop, pre_train=pre_train)

            (total_loss, loss_delay, loss_power, loss_phase,
            loss_path_length, loss_interaction) = masked_loss(
                delay_pred, power_pred, sin_pred, cos_pred, phase_pred,
                path_length_pred, interaction_logits, paths_out, path_lengths,
                interactions_out, pad_value=train_data.pad_value,
                interaction_weight=config.get("interaction_weight", 0.1)
            )

            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()
            
            path_length_rmse = compute_stop_metrics(path_length_pred.detach().squeeze(-1),
                                                    path_lengths)

            train_losses.append(total_loss.item())
            train_loss_delay.append(loss_delay.item())
            train_loss_power.append(loss_power.item())
            train_loss_phase.append(loss_phase.item())
            train_loss_path_length.append(loss_path_length.item())
            train_loss_interaction.append(loss_interaction.item())  # NEW
            train_path_length_rmse.append(path_length_rmse)
            current_lr = optimizer.param_groups[0]["lr"]
            pbar.set_postfix({
                "loss": f"{total_loss.item():.4f}",
                "delay": f"{loss_delay.item():.4f}",
                "power": f"{loss_power.item():.4f}",
                "phase": f"{loss_phase.item():.4f}",
                "inter": f"{loss_interaction.item():.4f}",  # NEW
                "path_rmse": f"{path_length_rmse:.4f}",
                "lr": f"{current_lr:.2e}"
            })

        avg_train_loss = np.mean(train_losses)
        avg_train_delay = np.mean(train_loss_delay)
        avg_train_power = np.mean(train_loss_power)
        avg_train_phase = np.mean(train_loss_phase)
        avg_train_path_length = np.mean(train_loss_path_length)
        avg_train_interaction = np.mean(train_loss_interaction)  # NEW
        avg_train_path_length_rmse = np.mean(train_path_length_rmse)
        scheduler.step()
        # -------------------- VALIDATION --------------------
        model.eval()

        with torch.no_grad():
            pbar = tqdm(val_loader, desc=f"Epoch {epoch} [Val]", leave=False)
            for prompts, paths, path_lengths, interactions, env, env_prop in pbar:  # NEW
                prompts = prompts.cuda()
                paths = paths.cuda()
                path_lengths = path_lengths.cuda()
                interactions = interactions.cuda()  # NEW
                env = env.cuda()
                env_prop = env_prop.cuda()


                paths_in = paths[:, :-1, :]
                interactions_in = interactions[:, :-1, :]

                paths_out = paths[:, 1:, :]
                interactions_out = interactions[:, 1:, :]  # NEW: shift targets

                (delay_pred, power_pred, sin_pred, cos_pred, phase_pred,
                path_length_pred, interaction_logits) = model(prompts, paths_in,interactions_in, env, env_prop, pre_train=pre_train)

                (total_loss, loss_delay, loss_power, loss_phase,
                loss_path_length, loss_interaction) = masked_loss(
                    delay_pred, power_pred, sin_pred, cos_pred, phase_pred,
                    path_length_pred, interaction_logits, paths_out, path_lengths,
                    interactions_out, pad_value=train_data.pad_value,
                    interaction_weight=config.get("interaction_weight", 0.1)
                )

                path_length_rmse = compute_stop_metrics(path_length_pred.detach().squeeze(-1),
                                                    path_lengths)

                val_losses.append(total_loss.item())
                val_loss_delay.append(loss_delay.item())
                val_loss_power.append(loss_power.item())
                val_loss_phase.append(loss_phase.item())
                val_loss_path_length.append(loss_path_length.item())
                val_loss_interaction.append(loss_interaction.item())  # NEW
                val_path_length_rmse.append(path_length_rmse)

                pbar.set_postfix({
                    "val_loss": f"{total_loss.item():.4f}",
                    "inter": f"{loss_interaction.item():.4f}",  # NEW

                })

        avg_val_loss = np.mean(val_losses)
        avg_val_delay = np.mean(val_loss_delay)
        avg_val_power = np.mean(val_loss_power)
        avg_val_phase = np.mean(val_loss_phase)
        avg_val_path_length = np.mean(val_loss_path_length)
        avg_val_interaction = np.mean(val_loss_interaction)  # NEW
        avg_val_path_length_rmse = np.mean(val_path_length_rmse)

        # scheduler.step(avg_val_loss)
        current_lr = optimizer.param_groups[0]["lr"]

        # -------------------- CHECKPOINT SAVING --------------------
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_val_loss': torch.tensor(best_val_loss),
            }, checkpoint_path)
            print(f"  ✓ Best checkpoint saved (val_loss: {best_val_loss:.4f})")

        if config.get("USE_WANDB", False):
            import wandb
            wandb.log({
                "train_loss": avg_train_loss,
                "train_loss_delay": avg_train_delay,
                "train_loss_power": avg_train_power,
                "train_loss_phase": avg_train_phase,
                "train_loss_path_length": avg_train_path_length,
                "train_loss_interaction": avg_train_interaction,  # NEW
                "train_path_length_rmse": avg_train_path_length_rmse,
                "val_loss": avg_val_loss,
                "val_loss_delay": avg_val_delay,
                "val_loss_power": avg_val_power,
                "val_loss_phase": avg_val_phase,
                "val_loss_path_length": avg_val_path_length,
                "val_loss_interaction": avg_val_interaction,  # NEW
                "val_path_length_rmse": avg_val_path_length_rmse,
                "epoch": epoch,
                "lr": current_lr,
            })

        print(f"\nEpoch {epoch:02d}")
        print(f"  Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")
        print(f"    Delay: {avg_train_delay:.4f} (val: {avg_val_delay:.4f})")
        print(f"    Power: {avg_train_power:.4f} (val: {avg_val_power:.4f})")
        print(f"    Phase: {avg_train_phase:.4f} (val: {avg_val_phase:.4f})")
        print(f"    Interaction: {avg_train_interaction:.4f} (val: {avg_val_interaction:.4f})")  # NEW
        print(f"    PathLength: {avg_train_path_length:.4f} (val: {avg_val_path_length:.4f})")  # NEW

        print(f"  LR: {current_lr:.3e}")



# %%




# Train


# %% [markdown]
#
# evaluate_generation(train_loader)
#

# %%
def load_best_checkpoint(model, checkpoint_path="checkpoints2/best_model_checkpoint.pth"):
    """
    Load the best model checkpoint saved during training.

    Args:
        model: The model instance to load the checkpoint into
        checkpoint_path: Path to the checkpoint file

    Returns:
        epoch: Epoch at which best checkpoint was saved
        best_val_loss: Best validation loss achieved
    """
    if not os.path.exists(checkpoint_path):
        print(f"Warning: Checkpoint not found at {checkpoint_path}")
        return None, None

    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    epoch = checkpoint['epoch']
    best_avg_val_loss = checkpoint['best_val_loss']

    print(f"✓ Loaded best checkpoint from epoch {epoch} (val_loss: {best_avg_val_loss:.4f})")
    return epoch, best_avg_val_loss
# # torch.serialization.add_safe_globals([np._core.multiarray.scalar])
# # torch.serialization.add_safe_globals([np.dtype])

dataset = dm.load(scenario)
train_data  = MySeqDataLoader(dataset, train=True, split_by="user", sort_by="power")
train_loader = torch.utils.data.DataLoader(
    dataset     = train_data,
    batch_size  = config['BATCH_SIZE'],
    shuffle     = True,
    collate_fn= train_data.collate_fn
    )
val_data  = MySeqDataLoader(dataset, train=False, split_by="user", sort_by="power")
val_loader = torch.utils.data.DataLoader(
    dataset     = val_data,
    batch_size  = config['BATCH_SIZE'],
    shuffle     = False,
    collate_fn= val_data.collate_fn
    )

# # Load best checkpoint for inference/evaluation
best_epoch, best_loss = load_best_checkpoint(model, checkpoint_path=base_model_checkpoint_path)

train_with_interactions(model, config)
# # %%
# checkpoint_path

# # %%
print(evaluate_model(model, val_loader))


# %%
# show_example(model, val_loader, sample_index=24)